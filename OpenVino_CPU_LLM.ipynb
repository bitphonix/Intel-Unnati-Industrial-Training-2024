{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMHCNm1BhL3-",
        "outputId": "b553c49c-ea59-4d52-f46f-cdd1d55ab443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
        "\n",
        "%pip install -Uq pip\n",
        "%pip uninstall -q -y optimum optimum-intel\n",
        "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
        "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
        "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
        "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
        "\"torch>=2.1\"\\\n",
        "\"datasets\" \\\n",
        "\"accelerate\"\\\n",
        "\"gradio>=4.19\"\\\n",
        "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.40\" \"bitsandbytes\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "# fetch model configuration\n",
        "\n",
        "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
        "config_dst_path = Path(\"llm_config.py\")\n",
        "\n",
        "if not config_dst_path.exists():\n",
        "    if config_shared_path.exists():\n",
        "        try:\n",
        "            os.symlink(config_shared_path, config_dst_path)\n",
        "        except Exception:\n",
        "            shutil.copy(config_shared_path, config_dst_path)\n",
        "    else:\n",
        "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
        "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(r.text)\n",
        "elif not os.path.islink(config_dst_path):\n",
        "    print(\"LLM config will be updated\")\n",
        "    if config_shared_path.exists():\n",
        "        shutil.copy(config_shared_path, config_dst_path)\n",
        "    else:\n",
        "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
        "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(r.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uStUnkF6h7LA",
        "outputId": "5595a9e6-c60c-4a0f-d820-4bf6399e6942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM config will be updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* zephyr-7b-beta - not enough ram\n",
        "* qwen2-0.5b-instruct - working\n",
        "* qwen2-1.5b-instruct - working\n",
        "* mistral-7b - restricted access\n",
        "* tiny-llama-1b-chat - working\n",
        "* gemma-2b-it - restricted access\n",
        "* notus-7b-v1 - not enough ram  \n",
        "* neural-chat-7b-v3-1\n",
        "* llama-2-chat-7b\n",
        "* llama-3-8b-instruct\n",
        "* gemma-7b-it\n",
        "* mpt-7b-chat\n",
        "* chatglm2-6b\n",
        "* qwen-7b-chat\n",
        "* red-pajama-3b-chat\n",
        "* phi-3-mini-instruct - not enough ram  "
      ],
      "metadata": {
        "id": "vus17TPj09U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_config import SUPPORTED_LLM_MODELS\n",
        "import ipywidgets as widgets\n",
        "\n",
        "model_language = \"English\"\n",
        "model_id = \"qwen2-0.5b-instruct\" #qwen2-0.5b-instruct\n",
        "model_configuration = SUPPORTED_LLM_MODELS[model_language][model_id]"
      ],
      "metadata": {
        "id": "h46W40WciENP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_fp16_model = False\n",
        "prepare_int8_model = False\n",
        "prepare_int4_model = True #False\n",
        "enable_awq = False\n",
        "\n",
        "device = \"CPU\"\n",
        "model_to_run = \"INT4\" # \"INT4\" \"INT8\", \"FP16\""
      ],
      "metadata": {
        "id": "cbY8dqUZi1Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pt_model_id = model_configuration[\"model_id\"]\n",
        "pt_model_name = model_id.split(\"-\")[0]\n",
        "fp16_model_dir = Path(model_id) / \"FP16\"\n",
        "int8_model_dir = Path(model_id) / \"INT8_compressed_weights\"\n",
        "int4_model_dir = Path(model_id) / \"INT4_compressed_weights\"\n",
        "\n",
        "\n",
        "def convert_to_fp16():\n",
        "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16\".format(pt_model_id)\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
        "    print(\"Export command:\")\n",
        "    print(export_command)\n",
        "    # display(Markdown(\"**Export command:**\"))\n",
        "    # display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "def convert_to_int8():\n",
        "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
        "    print(\"Export command:\")\n",
        "    print(export_command)\n",
        "    # display(Markdown(\"**Export command:**\"))\n",
        "    # display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "def convert_to_int4():\n",
        "    compression_configs = {\n",
        "        \"zephyr-7b-beta\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"mistral-7b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"minicpm-2b-dpo\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"gemma-2b-it\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"notus-7b-v1\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"neural-chat-7b-v3-1\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"llama-2-chat-7b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"llama-3-8b-instruct\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"gemma-7b-it\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"chatglm2-6b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.72,\n",
        "        },\n",
        "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
        "        \"red-pajama-3b-chat\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.5,\n",
        "        },\n",
        "        \"default\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    model_compression_params = compression_configs.get(model_id, compression_configs[\"default\"])\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_configuration.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
        "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
        "    if model_compression_params[\"sym\"]:\n",
        "        int4_compression_args += \" --sym\"\n",
        "    if enable_awq:\n",
        "        int4_compression_args += \" --awq --dataset wikitext2 --num-samples 128\"\n",
        "    export_command_base += int4_compression_args\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
        "    print(\"Export command:\")\n",
        "    print(export_command)\n",
        "    # display(Markdown(\"**Export command:**\"))\n",
        "    # display(Markdown(f\"`{export_command}`\"))\n",
        "    ! $export_command\n",
        "\n",
        "\n",
        "if prepare_fp16_model:\n",
        "    convert_to_fp16()\n",
        "if prepare_int8_model:\n",
        "    convert_to_int8()\n",
        "if prepare_int4_model:\n",
        "    convert_to_int4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IBlwQ2RiyTH",
        "outputId": "beb8471b-5e07-4058-8956-27fe23a5589d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export command:\n",
            "optimum-cli export openvino --model Qwen/Qwen2-0.5B-Instruct --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 qwen2-0.5b-instruct/INT4_compressed_weights\n",
            "2024-06-28 18:15:00.665037: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-28 18:15:00.665097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-28 18:15:00.668142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-28 18:15:00.684571: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-28 18:15:03.441832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "config.json: 100% 659/659 [00:00<00:00, 1.06MB/s]\n",
            "Framework not specified. Using pt to export the model.\n",
            "model.safetensors: 100% 988M/988M [00:14<00:00, 69.8MB/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 988kB/s]\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 5.65MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 26.9MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 20.8MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 22.3MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Using framework PyTorch: 2.3.0+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
            "/usr/local/lib/python3.10/dist-packages/optimum/exporters/onnx/model_patcher.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_len > self.max_seq_len_cached:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py:667: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
            "\u001b[2KMixed-Precision assignment \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m168/168\u001b[0m • \u001b[36m0:00:04\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO:nncf:Statistics of the bitwidth distribution:\n",
            "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
            "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
            "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
            "│              8 │ 43% (81 / 169)              │ 21% (80 / 168)                         │\n",
            "├────────────────┼─────────────────────────────┼────────────────────────────────────────┤\n",
            "│              4 │ 57% (88 / 169)              │ 79% (88 / 168)                         │\n",
            "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
            "\u001b[2KApplying Weight Compression \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m169/169\u001b[0m • \u001b[36m0:00:17\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hReplacing `(?!\\S)` pattern to `(?:$|[^\\S])` in RegexSplit operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
        "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if fp16_weights.exists():\n",
        "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
        "    if compressed_weights.exists():\n",
        "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    if compressed_weights.exists() and fp16_weights.exists():\n",
        "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRUDTvHxjG9N",
        "outputId": "823c610e-2ae0-4600-eb45-39d54edbbca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT4 compressed weights is 358.86 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model"
      ],
      "metadata": {
        "id": "EWV2eDbBjYRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from optimum.intel.openvino import OVModelForCausalLM\n",
        "\n",
        "if model_to_run == \"INT4\":\n",
        "    model_dir = int4_model_dir\n",
        "elif model_to_run == \"INT8\":\n",
        "    model_dir = int8_model_dir\n",
        "else:\n",
        "    model_dir = fp16_model_dir\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "if \"GPU\" in device and \"qwen2-7b-instruct\" in model_id:\n",
        "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
        "\n",
        "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
        "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
        "if model_id == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and device in [\"GPU\", \"AUTO\"]:\n",
        "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
        "\n",
        "model_name = model_configuration[\"model_id\"]\n",
        "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device=device,\n",
        "    ov_config=ov_config,\n",
        "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUAgTPnFjZnE",
        "outputId": "150e9e75-24e8-4f9e-aab0-8c5041807058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from qwen2-0.5b-instruct/INT4_compressed_weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "test_string = \"What is GenAI?\"\n",
        "input_tokens = tok(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "answer = ov_model.generate(**input_tokens, max_new_tokens=100)\n",
        "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3x0B7orjaNT",
        "outputId": "885d2856-c7db-484f-f1b8-1f83af98147e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is GenAI? It’s a new breed of AI that uses artificial intelligence to create highly complex systems. GenAI was created by the University of California at Berkeley, and it's designed to be used for applications such as autonomous vehicles, healthcare, and finance.\n",
            "The system uses deep learning algorithms to analyze large amounts of data and make predictions about future events. The company says that GenAI can handle billions of transactions per second and process images in real-time.\n",
            "GenAI has received a lot of attention recently, with many investors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Save the model"
      ],
      "metadata": {
        "id": "ud3XIlwvpCdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model for faster loading later\n",
        "ov_model.save_pretrained(\"qwen2-0.5b-instruct-ov\")\n",
        "tok.save_pretrained(\"qwen2-0.5b-instruct-ov\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4XmgrZao85B",
        "outputId": "4f5d3c30-a72a-43e7-8d6e-2006509dc58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('qwen2-0.5b-instruct-ov/tokenizer_config.json',\n",
              " 'qwen2-0.5b-instruct-ov/special_tokens_map.json',\n",
              " 'qwen2-0.5b-instruct-ov/vocab.json',\n",
              " 'qwen2-0.5b-instruct-ov/merges.txt',\n",
              " 'qwen2-0.5b-instruct-ov/added_tokens.json',\n",
              " 'qwen2-0.5b-instruct-ov/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a Saved Model"
      ],
      "metadata": {
        "id": "Qyug06BDpYVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a saved model\n",
        "model = OVModelForCausalLM.from_pretrained(\"qwen2-0.5b-instruct-ov\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"qwen2-0.5b-instruct-ov\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y602944rpcKe",
        "outputId": "097b7ab3-34c7-4fb9-cc2d-b6d5f9eb017e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compiling the model to CPU ...\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "test_string = \"What is the process for the impeachment of the President of India?\"\n",
        "input_tokens = tokenizer(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "answer = model.generate(**input_tokens, max_new_tokens=100)\n",
        "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElCnjRP3ph6i",
        "outputId": "f59832b2-a9df-4376-913c-86475f3170fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the process for the impeachment of the President of India?\r\n",
            "\r\n",
            "The process for impeaching the President of India involves a formal complaint being made to the Supreme Court, which then issues an order requiring the President to answer questions and provide evidence. If the President refuses to appear or does not answer questions in the prescribed time period, the case goes to the Lok Sabha (upper house) for further hearing. In the event that the President still fails to appear or provide evidence, the House of Representatives can vote on the impeachment motion. If the House of Representatives votes against\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "test_string = \"What is the process for the impeachment of the President of India?\"\n",
        "input_tokens = tokenizer(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "answer = model.generate(**input_tokens, max_new_tokens=200)\n",
        "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFnzL8ELGjLg",
        "outputId": "a81cfcc7-b9bb-41c4-8c45-1d814ff90ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the process for the impeachment of the President of India? The process for impeaching the President of India is as follows:\n",
            "\n",
            "1. The Congress party leader or any other party with a majority in the Legislative Council issues a resolution to impeach the President.\n",
            "2. The Prime Minister, who is responsible for implementing policies and regulations, decides whether to initiate an investigation into the allegations made against the President.\n",
            "3. If the Prime Minister decides to initiate an investigation, he/she submits the chargesheet to the Supreme Court within 7 days.\n",
            "4. The Supreme Court then conducts an inquiry on the chargesheet and decides if there are sufficient grounds for impeachment. If not, it sends the case back to the Prime Minister for further investigation.\n",
            "5. If there are enough grounds for impeachment, the Prime Minister can decide to initiate an impeachment trial. In such cases, the House of Councillors will vote on the impeachment bill.\n",
            "6. If the House of Councillors votes in favor of impeachment, the President's resignation will be declared unconstitutional.\n",
            "\n",
            "In summary, the\n"
          ]
        }
      ]
    }
  ]
}